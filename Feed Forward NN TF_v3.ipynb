{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FCLayer(size_inp, inp, nodes):\n",
    "    W = tf.random_normal(shape=(size_inp,nodes), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "    #    W = tf.random_normal(shape=(tf.shape(inp)[1],nodes), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "    W = tf.Variable(W)\n",
    "#    bias = tf.random_normal(shape=(1,nodes), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "#    bias = tf.Variable(bias)\n",
    "    out = tf.matmul(inp,W)\n",
    "    \n",
    "    return W, out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reLULayer(inp):\n",
    "    out = tf.maximum(0.0,inp)        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Layer(size_inp, inp, nodes):\n",
    "    W, outFC = FCLayer(size_inp, inp, nodes)\n",
    "    outReLU = reLULayer(outFC)\n",
    "    return W, outReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(size_inp, inp):\n",
    "    W1, out1 = Layer(size_inp, inp, 4)\n",
    "    \n",
    "    W2, out2 = Layer(4, tf.nn.sigmoid(out1), 3)\n",
    "    \n",
    "    W3, out3 = Layer(3, tf.nn.sigmoid(out2), 2)\n",
    "    \n",
    "    return W1, W2, W3, out3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(size_X, X):\n",
    "#     return network(size_X, X)\n",
    "   return tf.nn.sigmoid(network(size_X, X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(Yhat):\n",
    "    predicted = tf.argmax(Yhat, dimension=1)\n",
    "    return predicted\n",
    "    #print output.eval(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(Yhat, Ytrue):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Yhat, Ytrue))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss):\n",
    "    learning_rate = 0.01\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 1000\n",
    "batch_size = 100\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 7 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu)\n",
    "    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu)\n",
    "    out = layers.fully_connected(fc2, n_classes, activation_fn=tf.nn.softmax)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x)\n",
    "print(pred.get_shape())\n",
    "# Define loss and optimizer\n",
    "# softmax = tf.nn.sparse_softmax_cross_entropy_with_logits(pred, y)\n",
    "\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(pred, y)\n",
    "cost = tf.reduce_mean(softmax)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Train Sequence\n",
    "train_X = np.random.randint(0, 2, size=(20000, n_input))\n",
    "train_Y = np.remainder(np.sum(train_X, axis=1),2)\n",
    "\n",
    "# new_X = np.ones((train_X.shape[0], train_X.shape[1]+1))\n",
    "# new_X[:, 1:] = train_X\n",
    "# train_X = new_X\n",
    "\n",
    "# Convert into one-hot vectors\n",
    "num_labels = len(np.unique(train_Y))\n",
    "new_Y = np.eye(num_labels)[train_Y]  # One liner trick!\n",
    "train_Y = new_Y\n",
    "\n",
    "X_size = train_X.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test Sequence\n",
    "test_X = np.random.randint(0, 2, size=(10000, n_input))\n",
    "test_Y = np.remainder(np.sum(test_X, axis=1),2)\n",
    "\n",
    "# new_X = np.ones((test_X.shape[0], test_X.shape[1]+1))\n",
    "# new_X[:, 1:] = test_X\n",
    "# test_X = new_X\n",
    "\n",
    "# Convert into one-hot vectors\n",
    "num_labels = len(np.unique(test_Y))\n",
    "new_Y = np.eye(num_labels)[test_Y]  # One liner trick!\n",
    "test_Y = new_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.693084955, Train Accuracy: 0.51955\n",
      "Epoch: 0101 cost= 0.688428044, Train Accuracy: 0.54965\n",
      "Epoch: 0201 cost= 0.685062289, Train Accuracy: 0.61915\n",
      "Epoch: 0301 cost= 0.680728257, Train Accuracy: 0.69225\n",
      "Epoch: 0401 cost= 0.675278544, Train Accuracy: 0.76285\n",
      "Epoch: 0501 cost= 0.667772770, Train Accuracy: 0.8029\n",
      "Epoch: 0601 cost= 0.656104565, Train Accuracy: 0.86285\n",
      "Epoch: 0701 cost= 0.637890637, Train Accuracy: 0.8735\n",
      "Epoch: 0801 cost= 0.610171139, Train Accuracy: 0.9155\n",
      "Epoch: 0901 cost= 0.566733122, Train Accuracy: 0.9546\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9619\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Loop over all batches\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c, s = sess.run([optimizer, cost, softmax], feed_dict={x: train_X, y: train_Y})\n",
    "        # Compute average loss\n",
    "        avg_cost += c\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:            \n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost) + \", Train Accuracy:\", accuracy.eval({x: train_X, y: train_Y}))\n",
    "#             print(sess.run(pred, feed_dict={x: test_X, y: test_Y}), test_Y)\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({x: test_X, y: test_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
